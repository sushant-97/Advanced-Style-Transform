{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 3144104,
          "sourceType": "datasetVersion",
          "datasetId": 1914240
        },
        {
          "sourceId": 3142192,
          "sourceType": "datasetVersion",
          "datasetId": 1913253
        }
      ],
      "dockerImageVersionId": 30498,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushant-97/Style-Transform-Dashtoon-Assignment-GenAI/blob/main/Final_dash_toon_style_transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In this notebook the AdaIn Style Transfer is emplemented from the original paper [Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization](https://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.pdf). It is based on this [github repository](https://github.com/irasin/Pytorch_AdaIN)."
      ],
      "metadata": {
        "id": "NtgGeBpJQ1Za"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NST originated with [Gatys et al.](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf), with the central concept being that the statistical characteristics (mean, variance, etc.) of feature maps derived from internal Conv layers, originally designed for pre-trained image recognition, can capture the stylistic essence of an image.\n",
        "\n",
        "While early approaches predominantly relied on optimization methods, the currenttly trend shifted towards utilizing feed-forward networks as generators in subsequent research. The DeNA article provides comprehensive details and comparisons of notable methods, sparing the need for an in-depth discussion here.\n",
        "\n",
        "In my implementation this time, I opted for the relatively straightforward Adain, and I'll provide a brief explanation here."
      ],
      "metadata": {
        "id": "5ytQJm54R15W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization\n"
      ],
      "metadata": {
        "id": "STk72F9KSYZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is characterized by the use of Adaptive Instance Normalization (Adain). To understand Adain, it's essential to first comprehend Instance Normalization.\n",
        "\n",
        "Instance normalization and Batch normalization are often compared, so what are the key differences?\n",
        "\n",
        "Let's consider the output of an intermediate layer in a typical CNN, with a size of (b, c, h, w). Now, let's summarize where each normalization method normalizes and what mean and variance are obtained:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   Batch Normalization (BN):\n",
        "      1.   Calculate mean and variance for b, h, w in (b, c, h, w).\n",
        "      2.   Obtain vectors of mean and variance with shape=c.\n",
        "\n",
        "2.   Instance Normalization (IN):\n",
        "    1. Calculate mean and variance for h, w in (b, c, h, w).\n",
        "    2. Obtain matrices of mean and variance with shape=(b, c).\n",
        "\n",
        "3.  Layer Normalization (LN, and a brief explanation of Layer Normalization):\n",
        "    1. Calculate mean and variance for c, h, w in (b, c, h, w).\n",
        "    2. Obtain vectors of mean and variance with shape=b.\n",
        "\n",
        "\n",
        "In the proposed method in this paper, a VGG-based AutoEncoder with an Adain layer in the middle is trained. Content and Style images are inputted into the Encoder, and the output feature maps are referred to as C_feature and S_feature, respectively. For each, mean and variance are calculated similar to the IN method. Finally, C_feature is normalized using the mean and variance of C_feature and then inverse-normalized using the mean and variance of S_feature. In other words, the mean and variance of C_feature are transformed to match those of S_feature. This is done based on the idea from Gatys et al. that the statistical information of feature maps can capture the artistic style.\n",
        "\n",
        "The original paper implemented this in Torch, but this time I implemented Adain from scratch in Pytorch. The code is available on GitHub. Feel free to check it out and give it a star if you find it helpful."
      ],
      "metadata": {
        "id": "YZoGxv3_SX7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results:\n",
        "I'll share some result images. Additionally, I've implemented a feature to adjust the artistic style, so please feel free to try that as well.\n"
      ],
      "metadata": {
        "id": "9AcpyZQ5TziT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torchvision import transforms\n",
        "from torchvision import models"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T18:19:49.220342Z",
          "iopub.execute_input": "2023-11-22T18:19:49.221076Z",
          "iopub.status.idle": "2023-11-22T18:19:49.227765Z",
          "shell.execute_reply.started": "2023-11-22T18:19:49.221031Z",
          "shell.execute_reply": "2023-11-22T18:19:49.226812Z"
        },
        "trusted": true,
        "id": "aCpoDGbLQ1Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils functions ans model implementation\n",
        "def calc_mean_std(features):\n",
        "    \"\"\"\n",
        "    :param features: shape of features -> [batch_size, c, h, w]\n",
        "    :return: features_mean, feature_s: shape of mean/std ->[batch_size, c, 1, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, c = features.size()[:2]\n",
        "    features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n",
        "    features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + 1e-6\n",
        "    return features_mean, features_std\n",
        "\n",
        "\n",
        "def adain(content_features, style_features):\n",
        "    \"\"\"\n",
        "    Adaptive Instance Normalization\n",
        "    :param content_features: shape -> [batch_size, c, h, w]\n",
        "    :param style_features: shape -> [batch_size, c, h, w]\n",
        "    :return: normalized_features shape -> [batch_size, c, h, w]\n",
        "    \"\"\"\n",
        "    content_mean, content_std = calc_mean_std(content_features)\n",
        "    style_mean, style_std = calc_mean_std(style_features)\n",
        "    normalized_features = style_std * (content_features - content_mean) / content_std + style_mean\n",
        "    return normalized_features\n",
        "\n",
        "\n",
        "class VGGEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
        "        self.slice1 = vgg[: 2]\n",
        "        self.slice2 = vgg[2: 7]\n",
        "        self.slice3 = vgg[7: 12]\n",
        "        self.slice4 = vgg[12: 21]\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, images, output_last_feature=False):\n",
        "        h1 = self.slice1(images)\n",
        "        h2 = self.slice2(h1)\n",
        "        h3 = self.slice3(h2)\n",
        "        h4 = self.slice4(h3)\n",
        "        if output_last_feature:\n",
        "            return h4\n",
        "        else:\n",
        "            return h1, h2, h3, h4\n",
        "\n",
        "\n",
        "class RC(nn.Module):\n",
        "    \"\"\"A wrapper of ReflectionPad2d and Conv2d\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, pad_size=1, activated=True):\n",
        "        super().__init__()\n",
        "        self.pad = nn.ReflectionPad2d((pad_size, pad_size, pad_size, pad_size))\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
        "        self.activated = activated\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.pad(x)\n",
        "        h = self.conv(h)\n",
        "        if self.activated:\n",
        "            return F.relu(h)\n",
        "        else:\n",
        "            return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rc1 = RC(512, 256, 3, 1)\n",
        "        self.rc2 = RC(256, 256, 3, 1)\n",
        "        self.rc3 = RC(256, 256, 3, 1)\n",
        "        self.rc4 = RC(256, 256, 3, 1)\n",
        "        self.rc5 = RC(256, 128, 3, 1)\n",
        "        self.rc6 = RC(128, 128, 3, 1)\n",
        "        self.rc7 = RC(128, 64, 3, 1)\n",
        "        self.rc8 = RC(64, 64, 3, 1)\n",
        "        self.rc9 = RC(64, 3, 3, 1, False)\n",
        "\n",
        "    def forward(self, features):\n",
        "        h = self.rc1(features)\n",
        "        h = F.interpolate(h, scale_factor=2)\n",
        "        h = self.rc2(h)\n",
        "        h = self.rc3(h)\n",
        "        h = self.rc4(h)\n",
        "        h = self.rc5(h)\n",
        "        h = F.interpolate(h, scale_factor=2)\n",
        "        h = self.rc6(h)\n",
        "        h = self.rc7(h)\n",
        "        h = F.interpolate(h, scale_factor=2)\n",
        "        h = self.rc8(h)\n",
        "        h = self.rc9(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vgg_encoder = VGGEncoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def generate(self, content_images, style_images, alpha=1.0):\n",
        "        self.vgg_encode = self.vgg_encoder\n",
        "        content_features = self.vgg_encoder(content_images, output_last_feature=True)\n",
        "        style_features = self.vgg_encoder(style_images, output_last_feature=True)\n",
        "        t = adain(content_features, style_features)\n",
        "        t = alpha * t + (1 - alpha) * content_features\n",
        "        out = self.decoder(t)\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_content_loss(out_features, t):\n",
        "        return F.mse_loss(out_features, t)\n",
        "\n",
        "    @staticmethod\n",
        "    def calc_style_loss(content_middle_features, style_middle_features):\n",
        "        loss = 0\n",
        "        for c, s in zip(content_middle_features, style_middle_features):\n",
        "            c_mean, c_std = calc_mean_std(c)\n",
        "            s_mean, s_std = calc_mean_std(s)\n",
        "            loss += F.mse_loss(c_mean, s_mean) + F.mse_loss(c_std, s_std)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, content_images, style_images, alpha=1.0, lam=10):\n",
        "        content_features = self.vgg_encoder(content_images, output_last_feature=True)\n",
        "        style_features = self.vgg_encoder(style_images, output_last_feature=True)\n",
        "        t = adain(content_features, style_features)\n",
        "        t = alpha * t + (1 - alpha) * content_features\n",
        "        out = self.decoder(t)\n",
        "\n",
        "        output_features = self.vgg_encoder(out, output_last_feature=True)\n",
        "        output_middle_features = self.vgg_encoder(out, output_last_feature=False)\n",
        "        style_middle_features = self.vgg_encoder(style_images, output_last_feature=False)\n",
        "\n",
        "        loss_c = self.calc_content_loss(output_features, t)\n",
        "        loss_s = self.calc_style_loss(output_middle_features, style_middle_features)\n",
        "        loss = loss_c + lam * loss_s\n",
        "        return loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T18:19:49.229428Z",
          "iopub.execute_input": "2023-11-22T18:19:49.229729Z",
          "iopub.status.idle": "2023-11-22T18:19:49.256471Z",
          "shell.execute_reply.started": "2023-11-22T18:19:49.229704Z",
          "shell.execute_reply": "2023-11-22T18:19:49.255620Z"
        },
        "trusted": true,
        "id": "dP0wpsccQ1Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=512),\n",
        "    transforms.RandomCrop(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "def denorm(tensor, device):\n",
        "    std = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1).to(device)\n",
        "    mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1).to(device)\n",
        "    res = torch.clamp(tensor * std + mean, 0, 1)\n",
        "    return res\n",
        "\n",
        "\n",
        "class ContentStyleDataset(Dataset):\n",
        "    def __init__(self, content_dir, style_dir, num_range, transform=transform):\n",
        "        content_images = glob.glob(os.path.join(content_dir, \"*.jpg\"))\n",
        "        style_images = glob.glob(os.path.join(style_dir, \"*.jpg\"))\n",
        "\n",
        "        self.images_pairs = list(zip(content_images, style_images))[num_range[0]:num_range[1]]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_pairs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        content_image, style_image = self.images_pairs[index]\n",
        "        content_image = Image.open(content_image).convert(\"RGB\")\n",
        "        style_image = Image.open(style_image).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            content_image = self.transform(content_image)\n",
        "            style_image = self.transform(style_image)\n",
        "        return content_image, style_image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T18:19:49.257750Z",
          "iopub.execute_input": "2023-11-22T18:19:49.258030Z",
          "iopub.status.idle": "2023-11-22T18:19:49.272217Z",
          "shell.execute_reply.started": "2023-11-22T18:19:49.258006Z",
          "shell.execute_reply": "2023-11-22T18:19:49.271437Z"
        },
        "trusted": true,
        "id": "-HPRKDdnQ1Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "learning_rate = 5e-5\n",
        "train_content_dir ='/kaggle/input/coco-wikiart-nst-dataset-512-100000/content'\n",
        "train_style_dir = '/kaggle/input/coco-wikiart-nst-dataset-512-100000/style'\n",
        "test_content_dir = '/kaggle/input/coco-wikiart-nst-dataset-512-100000/content'\n",
        "test_style_dir = '/kaggle/input/coco-wikiart-nst-dataset-512-100000/style'\n",
        "\n",
        "\n",
        "loss_dir = \"loss\"\n",
        "model_state_dir = \"model_state\"\n",
        "\n",
        "if not os.path.exists(loss_dir):\n",
        "    os.mkdir(loss_dir)\n",
        "if not os.path.exists(model_state_dir):\n",
        "    os.mkdir(model_state_dir)\n",
        "\n",
        "# set device on GPU if available, else CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "# to speed-up the training we will be using only 32000 context and style pairs of images\n",
        "num_train_range = (0, 32000)\n",
        "\n",
        "# prepare dataset and dataLoader\n",
        "train_dataset = ContentStyleDataset(train_content_dir, train_style_dir, num_range=num_train_range)\n",
        "iters = len(train_dataset)\n",
        "print(f'Length of train image pairs: {iters}')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# set model and optimizer\n",
        "model = Model()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# start training\n",
        "loss_list = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f'Start {epoch} epoch')\n",
        "    for i, (content, style) in tqdm(enumerate(train_loader, 1)):\n",
        "        content = content.to(device)\n",
        "        style = style.to(device)\n",
        "        loss = model(content, style)\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    print(f'[Epoch {epoch} Loss: {loss.item()}')\n",
        "    torch.save(model.state_dict(), f'{model_state_dir}/{epoch}_epoch.pth')\n",
        "\n",
        "# plot training loss\n",
        "plt.plot(range(len(loss_list)), loss_list)\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title('train loss')\n",
        "plt.savefig(f'{loss_dir}/train_loss.png')\n",
        "with open(f'{loss_dir}/loss_log.txt', 'w') as f:\n",
        "    for l in loss_list:\n",
        "        f.write(f'{l}\\n')\n",
        "print(f'Loss saved in {loss_dir}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-22T18:19:49.429764Z",
          "iopub.execute_input": "2023-11-22T18:19:49.430579Z"
        },
        "trusted": true,
        "id": "QS68iWysQ1Zf",
        "outputId": "da3200ce-05ec-4951-fed2-0d8ba881e8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Length of train image pairs: 32000\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n 42%|████▏     | 233M/548M [00:00<00:00, 342MB/s]  ",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "\n",
        "model = model.eval()\n",
        "\n",
        "# take 5 image pairs for the test set\n",
        "num_test_range = (32001, 32006)\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                 transforms.ToTensor(),\n",
        "                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                      std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "test_dataset = ContentStyleDataset(test_content_dir, test_style_dir, transform=test_transform, num_range=num_test_range)\n",
        "iters = len(test_dataset)\n",
        "print(f'Length of test image pairs: {iters}')\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=iters, shuffle=False)\n",
        "\n",
        "for i, (content, style) in tqdm(enumerate(test_loader, 1)):\n",
        "    content = content.to(device)\n",
        "    style = style.to(device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(content, style)\n",
        "        content = denorm(content, device).detach().cpu()\n",
        "        style = denorm(style, device).detach().cpu()\n",
        "        out = denorm(out, device).detach().cpu()\n",
        "        res = torch.cat([content, style, out], dim=0)\n",
        "        grid_img = make_grid(res, nrow=iters)\n",
        "        plt.figure(figsize=(20, 12))\n",
        "        plt.imshow(grid_img.permute(1, 2, 0))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "-3NDlNF9Q1Zg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}