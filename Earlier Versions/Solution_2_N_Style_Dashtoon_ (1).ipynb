{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HDRYfG1tgBIT",
        "hVwdfA9GYAjh",
        "SF95d-Qef5nH",
        "lrT8E-Fpf8-J"
      ],
      "authorship_tag": "ABX9TyMp3V/Gygkk5FP0H8YPTzzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushant-97/Style-Transform/blob/main/Solution_2_N_Style_Dashtoon_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N Style Transform\n",
        "based on [A Learned Representation For Artistic Style](https://arxiv.org/abs/1610.07629)"
      ],
      "metadata": {
        "id": "YTPXn37zWVdx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0oFDNN4WOwH"
      },
      "outputs": [],
      "source": [
        "# Extended the code from\n",
        "# Fast Neural Style Transform library by adding Conditional\n",
        "# https://github.com/pytorch/examples/tree/main/fast_neural_style\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjnOYuDaX_bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "HDRYfG1tgBIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Utility Code.\"\"\"\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "MEAN = (0.485, 0.456, 0.406)\n",
        "STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "normalize = T.Normalize(mean=MEAN, std=STD)\n",
        "denormalize = T.Normalize(mean=[-m/s for m, s in zip(MEAN, STD)],\n",
        "                          std=[1/std for std in STD])\n",
        "\n",
        "\n",
        "def get_transforms(imsize=None, cropsize=None, cencrop=False):\n",
        "    \"\"\"Get the transforms.\"\"\"\n",
        "    transformer = []\n",
        "    if imsize:\n",
        "        transformer.append(T.Resize(imsize))\n",
        "    if cropsize:\n",
        "        if cencrop:\n",
        "            transformer.append(T.CenterCrop(cropsize))\n",
        "        else:\n",
        "            transformer.append(T.RandomCrop(cropsize))\n",
        "\n",
        "    transformer.append(T.ToTensor())\n",
        "    transformer.append(normalize)\n",
        "    return T.Compose(transformer)\n",
        "\n",
        "\n",
        "def imload(path, imsize=None, cropsize=None, cencrop=False):\n",
        "    \"\"\"Load a image.\"\"\"\n",
        "    transformer = get_transforms(imsize=imsize,\n",
        "                                 cropsize=cropsize,\n",
        "                                 cencrop=cencrop)\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    return transformer(image).unsqueeze(0)\n",
        "\n",
        "\n",
        "def imsave(image, save_path):\n",
        "    \"\"\"Save a image.\"\"\"\n",
        "    image = denormalize(torchvision.utils.make_grid(image)).clamp_(0.0, 1.0)\n",
        "    torchvision.utils.save_image(image, save_path)\n",
        "    return None\n",
        "\n",
        "\n",
        "class ImageDataset:\n",
        "    \"\"\"Image Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, dir_path):\n",
        "        \"\"\"Init.\"\"\"\n",
        "        self.images = sorted(list(dir_path.glob('*.jpg')))\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the Number of data sampels.\"\"\"\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get Image and Index.\"\"\"\n",
        "        img = Image.open(self.images[index]).convert('RGB')\n",
        "        return img, index\n",
        "\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Data Processor.\"\"\"\n",
        "\n",
        "    def __init__(self, imsize=256, cropsize=240, cencrop=False):\n",
        "        \"\"\"Init.\"\"\"\n",
        "        self.transforms = get_transforms(imsize=imsize,\n",
        "                                         cropsize=cropsize,\n",
        "                                         cencrop=cencrop)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        \"\"\"Process the batch.\"\"\"\n",
        "        images, indices = list(zip(*batch))\n",
        "\n",
        "        inputs = torch.stack([self.transforms(image) for image in images])\n",
        "        return inputs, indices\n"
      ],
      "metadata": {
        "id": "M176cssTX_Xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function"
      ],
      "metadata": {
        "id": "hVwdfA9GYAjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Loss Function Code.\"\"\"\n",
        "# based on Pytorch official tutorial\n",
        "# https://pytorch.org/tutorials/advanced/neural_style_tutorial.html#:~:text=The%20style%20loss%20module%20is,_%7BXL%7D%20GXL.\n",
        "\n",
        "import torch\n",
        "from torch.nn.functional import mse_loss\n",
        "\n",
        "\n",
        "def calc_content_loss(features, targets, nodes):\n",
        "    \"\"\"Calculate Content Loss.\"\"\"\n",
        "    content_loss = 0\n",
        "    for node in nodes:\n",
        "        content_loss += mse_loss(features[node], targets[node])\n",
        "    return content_loss\n",
        "\n",
        "\n",
        "def gram(x):\n",
        "    \"\"\"Transfer a feature to gram matrix.\"\"\"\n",
        "    b, c, h, w = x.size()\n",
        "    f = x.flatten(2)\n",
        "    g = torch.bmm(f, f.transpose(1, 2))\n",
        "    return g.div(h*w)\n",
        "\n",
        "\n",
        "def calc_style_loss(features, targets, nodes):\n",
        "    \"\"\"Calcuate Gram Loss.\"\"\"\n",
        "    gram_loss = 0\n",
        "    for node in nodes:\n",
        "        gram_loss += mse_loss(gram(features[node]), gram(targets[node]))\n",
        "    return gram_loss\n",
        "\n",
        "\n",
        "def calc_tv_loss(x):\n",
        "    \"\"\"Calc Total Variation Loss.\"\"\"\n",
        "    tv_loss = torch.mean(torch.abs(x[:, :, :, :-1] - x[:, :, :, 1:]))\n",
        "    tv_loss += torch.mean(torch.abs(x[:, :, :-1, :] - x[:, :, 1:, :]))\n",
        "    return tv_loss\n"
      ],
      "metadata": {
        "id": "INQ25_zeYCSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "SF95d-Qef5nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Network Code.\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CIN(nn.Module):\n",
        "    \"\"\"Conditional Instance Norm.\"\"\"\n",
        "\n",
        "    def __init__(self, num_style, ch):\n",
        "        \"\"\"Init with number of style and channel.\"\"\"\n",
        "        super(CIN, self).__init__()\n",
        "        self.normalize = nn.InstanceNorm2d(ch, affine=False)\n",
        "        self.offset = nn.Parameter(0.01 * torch.randn(1, num_style, ch))\n",
        "        self.scale = nn.Parameter(1 + 0.01 * torch.randn(1, num_style, ch))\n",
        "\n",
        "    def forward(self, x, style_codes):\n",
        "        \"\"\"Forward func.\"\"\"\n",
        "        b, c, h, w = x.size()\n",
        "\n",
        "        x = self.normalize(x)\n",
        "\n",
        "        gamma = torch.sum(self.scale * style_codes, dim=1).view(b, c, 1, 1)\n",
        "        beta = torch.sum(self.offset * style_codes, dim=1).view(b, c, 1, 1)\n",
        "\n",
        "        x = x * gamma + beta\n",
        "\n",
        "        return x.view(b, c, h, w)\n",
        "\n",
        "\n",
        "class ConvWithCIN(nn.Module):\n",
        "    \"\"\"Convolution layer with CIN.\"\"\"\n",
        "\n",
        "    def __init__(self, num_style, in_ch, out_ch, stride, activation, ksize):\n",
        "        \"\"\"Init.\"\"\"\n",
        "        super(ConvWithCIN, self).__init__()\n",
        "        self.padding = nn.ReflectionPad2d(ksize // 2)\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, ksize, stride)\n",
        "\n",
        "        self.cin = CIN(num_style, out_ch)\n",
        "\n",
        "        # activatoin\n",
        "        if activation == \"relu\":\n",
        "            self.activation = nn.ReLU()\n",
        "\n",
        "        elif activation == \"linear\":\n",
        "            self.activation = lambda x: x\n",
        "\n",
        "    def forward(self, x, style_codes):\n",
        "        \"\"\"Forward func.\"\"\"\n",
        "        x = self.padding(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.cin(x, style_codes)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"ResidualBlock.\"\"\"\n",
        "\n",
        "    def __init__(self, num_style, in_ch, out_ch):\n",
        "        \"\"\"Init.\"\"\"\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = ConvWithCIN(num_style, in_ch, out_ch, 1, \"relu\", 3)\n",
        "        self.conv2 = ConvWithCIN(num_style, out_ch, out_ch, 1, \"linear\", 3)\n",
        "\n",
        "    def forward(self, x, style_codes):\n",
        "        \"\"\"Forward func.\"\"\"\n",
        "        out = self.conv1(x, style_codes)\n",
        "        out = self.conv2(out, style_codes)\n",
        "\n",
        "        return x + out\n",
        "\n",
        "\n",
        "class UpsamleBlock(nn.Module):\n",
        "    \"\"\"Upsampling Bloack.\"\"\"\n",
        "\n",
        "    def __init__(self, num_style, in_ch, out_ch):\n",
        "        \"\"\"Init.\"\"\"\n",
        "        super(UpsamleBlock, self).__init__()\n",
        "        self.conv = ConvWithCIN(num_style, in_ch, out_ch, 1, \"relu\", 3)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "    def forward(self, x, style_codes):\n",
        "        \"\"\"Forward func.\"\"\"\n",
        "        x = self.upsample(x)\n",
        "        x = self.conv(x, style_codes)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class StyleTransferNetwork(nn.Module):\n",
        "    \"\"\"Style Transfer Network.\"\"\"\n",
        "\n",
        "    def __init__(self, num_style=16):\n",
        "        \"\"\"Init.\"\"\"\n",
        "        super(StyleTransferNetwork, self).__init__()\n",
        "        self.conv1 = ConvWithCIN(num_style,  3, 32, 1, 'relu', 9)\n",
        "        self.conv2 = ConvWithCIN(num_style, 32, 64, 2, 'relu', 3)\n",
        "        self.conv3 = ConvWithCIN(num_style, 64, 128, 2, 'relu', 3)\n",
        "\n",
        "        self.residual1 = ResidualBlock(num_style, 128, 128)\n",
        "        self.residual2 = ResidualBlock(num_style, 128, 128)\n",
        "        self.residual3 = ResidualBlock(num_style, 128, 128)\n",
        "        self.residual4 = ResidualBlock(num_style, 128, 128)\n",
        "        self.residual5 = ResidualBlock(num_style, 128, 128)\n",
        "\n",
        "        self.upsampling1 = UpsamleBlock(num_style, 128, 64)\n",
        "        self.upsampling2 = UpsamleBlock(num_style, 64, 32)\n",
        "\n",
        "        self.conv4 = ConvWithCIN(num_style, 32, 3, 1, 'linear', 9)\n",
        "\n",
        "    def forward(self, x, style_codes):\n",
        "        \"\"\"Forward func.\"\"\"\n",
        "        x = self.conv1(x, style_codes)\n",
        "        x = self.conv2(x, style_codes)\n",
        "        x = self.conv3(x, style_codes)\n",
        "\n",
        "        x = self.residual1(x, style_codes)\n",
        "        x = self.residual2(x, style_codes)\n",
        "        x = self.residual3(x, style_codes)\n",
        "        x = self.residual4(x, style_codes)\n",
        "        x = self.residual5(x, style_codes)\n",
        "\n",
        "        x = self.upsampling1(x, style_codes)\n",
        "        x = self.upsampling2(x, style_codes)\n",
        "\n",
        "        x = self.conv4(x, style_codes)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pYJ4PYxlf7UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "lrT8E-Fpf8-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Pytorch Implementation Code.\n",
        "\n",
        "Reference: 'A Learned Representation for Artistic Style'\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from torch.optim import Adam\n",
        "from network import StyleTransferNetwork\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from utils import ImageDataset, DataProcessor, imsave, imload\n",
        "from loss import calc_content_loss, calc_style_loss, calc_tv_loss\n",
        "\n",
        "NUM_STYLE = 16\n",
        "\n",
        "\n",
        "def train(style_path, content_path,\n",
        "          style_weight=5.0, tv_weight=1e-5,\n",
        "          lr=1e-4, batch_size=8, iterations=40_000):\n",
        "    \"\"\"Train Network.\"\"\"\n",
        "    content_nodes = ['relu_3_3']\n",
        "    style_nodes = ['relu_1_2', 'relu_2_2', 'relu_3_3', 'relu_4_2']\n",
        "    return_nodes = {3: 'relu_1_2',\n",
        "                    8: 'relu_2_2',\n",
        "                    15: 'relu_3_3',\n",
        "                    22: 'relu_4_2'}\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    # data\n",
        "    content_dataset = ImageDataset(dir_path=Path(content_path))\n",
        "    style_dataset = ImageDataset(dir_path=Path(style_path))\n",
        "\n",
        "    data_processor = DataProcessor(imsize=256,\n",
        "                                   cropsize=240,\n",
        "                                   cencrop=False)\n",
        "    content_dataloader = DataLoader(dataset=content_dataset,\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    collate_fn=data_processor)\n",
        "    style_dataloader = DataLoader(dataset=style_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  collate_fn=data_processor)\n",
        "\n",
        "    # loss network\n",
        "    vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n",
        "    for param in vgg.parameters():\n",
        "        param.requires_grad = False\n",
        "    loss_network = create_feature_extractor(vgg, return_nodes).to(device)\n",
        "\n",
        "    # network\n",
        "    model = StyleTransferNetwork()\n",
        "    model.train()\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    losses = {'content': [], 'style': [], 'tv': [], 'total': []}\n",
        "    print(\"Start training...\")\n",
        "    for i in range(1, 1+iterations):\n",
        "        content_images, _ = next(iter(content_dataloader))\n",
        "        style_images, style_indices = next(iter(style_dataloader))\n",
        "\n",
        "        style_codes = torch.zeros(batch_size, NUM_STYLE, 1)\n",
        "        for b, s in enumerate(style_indices):\n",
        "            style_codes[b, s] = 1\n",
        "\n",
        "        content_images = content_images.to(device)\n",
        "        style_images = style_images.to(device)\n",
        "        style_codes = style_codes.to(device)\n",
        "\n",
        "        output_images = model(content_images, style_codes)\n",
        "\n",
        "        content_features = loss_network(content_images)\n",
        "        style_features = loss_network(style_images)\n",
        "        output_features = loss_network(output_images)\n",
        "\n",
        "        style_loss = calc_style_loss(output_features,\n",
        "                                     style_features,\n",
        "                                     style_nodes)\n",
        "        content_loss = calc_content_loss(output_features,\n",
        "                                         content_features,\n",
        "                                         content_nodes)\n",
        "        tv_loss = calc_tv_loss(output_images)\n",
        "\n",
        "        total_loss = content_loss \\\n",
        "            + style_loss * style_weight \\\n",
        "            + tv_loss * tv_weight\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses['content'].append(content_loss.item())\n",
        "        losses['style'].append(style_loss.item())\n",
        "        losses['tv'].append(tv_loss.item())\n",
        "        losses['total'].append(total_loss.item())\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            log = f\"iter.: {i}\"\n",
        "            for k, v in losses.items():\n",
        "                # calcuate a recent average value\n",
        "                avg = sum(v[-50:]) / 50\n",
        "                log += f\", {k}: {avg:1.4f}\"\n",
        "            print(log)\n",
        "\n",
        "    torch.save({\"state_dict\": model.state_dict()}, \"model.ckpt\")\n",
        "\n",
        "\n",
        "def evaluate(content_path, style_index):\n",
        "    \"\"\"Evaluate the network.\"\"\"\n",
        "    device = torch.device('cpu')\n",
        "    ckpt = torch.load('model.ckpt', map_location=device)\n",
        "\n",
        "    model = StyleTransferNetwork()\n",
        "    model.load_state_dict(ckpt['state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    content_image = imload(args.content_path, imsize=256)\n",
        "    # for all styles\n",
        "    if style_index == -1:\n",
        "        style_code = torch.eye(NUM_STYLE).unsqueeze(-1)\n",
        "        content_image = content_image.repeat(NUM_STYLE, 1, 1, 1)\n",
        "\n",
        "    # for specific style\n",
        "    elif style_index in range(NUM_STYLE):\n",
        "        style_code = torch.zeros(1, NUM_STYLE, 1)\n",
        "        style_code[:, style_index, :] = 1\n",
        "\n",
        "    else:\n",
        "        raise RuntimeError(\"Not expected style index\")\n",
        "\n",
        "    stylized_image = model(content_image, style_code)\n",
        "    imsave(stylized_image, 'stylized_images.jpg')\n",
        "    return None\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--mode', type=str, default='train',\n",
        "                        help=\"'train' | 'eval'\")\n",
        "    parser.add_argument('--style_path', type=str, default=None,\n",
        "                        help=\"Path of style image.\")\n",
        "    parser.add_argument('--content_path', type=str, default=None,\n",
        "                        help=\"Path of content image.\")\n",
        "    parser.add_argument('--style_index', type=int, default=0,\n",
        "                        help=\"Index for stylization, -1: all styles.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.mode == 'train':\n",
        "        train(args.style_path, args.content_path)\n",
        "\n",
        "    elif args.mode == 'eval':\n",
        "        evaluate(args.content_path, args.style_index)\n",
        "\n",
        "    else:\n",
        "        raise RuntimeError(\"Not exepcted mode\")\n"
      ],
      "metadata": {
        "id": "yS5Uxcsnf222"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}